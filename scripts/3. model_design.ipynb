{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference : \n",
    "\n",
    "1. https://github.com/tdeboissiere/DeepLearningImplementations/blob/master/pix2pix/src/model/models.py\n",
    "2. https://github.com/williamFalcon/pix2pix-keras/blob/master/pix2pix/networks/discriminator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Activation, Lambda, Reshape\n",
    "from keras.layers.convolutional import Conv2D, Deconv2D, ZeroPadding2D, Conv2DTranspose\n",
    "from keras.layers import Input, Concatenate, Merge\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "import keras.backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PatchGAN**\n",
    "\n",
    "PatchGAN은 GAN에서 Discriminator를 Patch 단위로 Loss를 연산할 수 있도록 디자인한 GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minb_disc(x):\n",
    "    diffs = K.expand_dims(x, 3) - K.expand_dims(K.permute_dimensions(x, [1, 2, 0]), 0)\n",
    "    abs_diffs = K.sum(K.abs(diffs), 2)\n",
    "    x = K.sum(K.exp(-abs_diffs), 2)\n",
    "\n",
    "    return x\n",
    "\n",
    "def get_nb_patch(img_dim, patch_size):\n",
    "    # 이미지를 patch 사이즈만큼 잘라, 몇 개가 나오는지 연산하는 함수\n",
    "    assert img_dim[0] % patch_size[0] == 0, \"patch_size does not divide height\"\n",
    "    assert img_dim[1] % patch_size[1] == 0, \"patch_size does not divide width\"\n",
    "    \n",
    "    nb_patch = (img_dim[0] // patch_size[0]) * (img_dim[1] // patch_size[1])\n",
    "    img_dim_disc = (patch_size[0], patch_size[1], img_dim[-1])\n",
    "    \n",
    "    return nb_patch, img_dim_disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DISCRIMINATOR(img_dim, patch_size, nb_filters=64):\n",
    "    \"\"\"\n",
    "    Creates the generator according to the specs in the paper below.\n",
    "    [https://arxiv.org/pdf/1611.07004v1.pdf][5. Appendix]\n",
    "    PatchGAN only penalizes structure at the scale of patches. This\n",
    "    discriminator tries to classify if each N x N patch in an\n",
    "    image is real or fake. We run this discriminator convolutationally\n",
    "    across the image, averaging all responses to provide\n",
    "    the ultimate output of D.\n",
    "    The discriminator has two parts. First part is the actual discriminator\n",
    "    seconds part we make it a PatchGAN by running each image patch through the model\n",
    "    and then we average the responses\n",
    "    Discriminator does the following:\n",
    "    1. Runs many pieces of the image through the network\n",
    "    2. Calculates the cost for each patch\n",
    "    3. Returns the avg of the costs as the output of the network\n",
    "    :param patch_dim: (channels, width, height) T\n",
    "    :param nb_patches:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    patch_dim = *patch_size, img_dim[2]\n",
    "    # Get the number of non overlapping patch and the size of input image to the discriminator\n",
    "    nb_patch, img_dim_disc = get_nb_patch(img_dim, patch_size)\n",
    "\n",
    "    # generate a list of inputs for the difference patches to the network\n",
    "    list_input = [Input(shape=patch_dim, name=\"patch_gan_input_{}\".format(i)) for i in range(nb_patch)]\n",
    "\n",
    "    # generate individual losses for each patch\n",
    "    patchblock = PATCHBLOCK(patch_dim,img_dim, nb_filters)\n",
    "    patches = [patchblock(patch) for patch in list_input]\n",
    "\n",
    "    # generate minibatch discriminator among patches\n",
    "    x, x_mbd = list(zip(*patches))\n",
    "    x_out = MINIBATCH_DISCRIMINATOR(list(x), list(x_mbd))\n",
    "\n",
    "    return Model(inputs=list_input, outputs=[x_out], name='discriminator')\n",
    "\n",
    "def PATCHBLOCK(patch_dim, img_dim, nb_filters):\n",
    "    # We have to build the discriminator dinamically because\n",
    "    # the size of the disc patches is dynamic\n",
    "    \n",
    "    # 의심스러운 구석 중 하나. patch_dim으로 바뀌어야 할 거 같은데... 그래서 바꾸었음\n",
    "    # 70 x 70 Discriminator architecture is:\n",
    "    # C64 - C128 - C256 - C512\n",
    "    #nb_conv = int(np.floor(np.log2(img_dim[1])))\n",
    "    nb_conv = int(np.floor(np.log2(patch_dim[1])-np.log2(4)))\n",
    "    list_filters = [nb_filters * min(8, (2 ** i)) for i in range(nb_conv)]\n",
    "    \n",
    "    # INPUT\n",
    "    input_layer = Input(shape=patch_dim)\n",
    "    # CONV 1\n",
    "    # Do first conv bc it is different from the rest\n",
    "    # paper skips batch norm for first layer\n",
    "    x = Conv2D(list_filters[0], (3, 3),\n",
    "                      strides=(2, 2), name=\"disc_conv2d_1\", padding=\"same\")(input_layer)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    # CONV 2 - CONV N\n",
    "    # do the rest of the convs based on the sizes from the filters\n",
    "    for i, filter_size in enumerate(list_filters[1:]):\n",
    "        name = 'disc_conv2d_{}'.format(i+2)\n",
    "        x = Conv2D(filter_size, (3, 3), strides=(2, 2), name=name, padding=\"same\")(x)\n",
    "        x = BatchNormalization(axis=-1)(x)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "\n",
    "    out_flat = Flatten()(x)\n",
    "    out = Dense(2, activation='softmax', name='disc_dense')(out_flat)\n",
    "\n",
    "    return Model(inputs=input_layer, outputs=[out,out_flat], name='patch_gan')\n",
    "\n",
    "def MINIBATCH_DISCRIMINATOR(x, x_mbd):\n",
    "    # merge layers if have multiple patches (aka perceptual loss)\n",
    "    if len(x) > 1:\n",
    "        x = Concatenate()(x)\n",
    "    else:\n",
    "        x = x[0]\n",
    "    # merge mbd if needed\n",
    "    # mbd = mini batch discrimination\n",
    "    # https://arxiv.org/pdf/1606.03498.pdf\n",
    "    if len(x_mbd) > 1:\n",
    "        x_mbd = Concatenate()(x_mbd)\n",
    "    else:\n",
    "        x_mbd = x_mbd[0]\n",
    "    \n",
    "    num_kernels = 100\n",
    "    dim_per_kernel = 5\n",
    "\n",
    "    x_mbd = Dense(num_kernels * dim_per_kernel, use_bias=False, activation=None)(x_mbd)\n",
    "    x_mbd = Reshape((num_kernels, dim_per_kernel))(x_mbd)\n",
    "    x_mbd = Lambda(minb_disc, output_shape=lambda shape : shape[:2], name='disc_mbd')(x_mbd)\n",
    "\n",
    "    x = Concatenate()([x,x_mbd])\n",
    "    return Dense(2, activation='softmax', name='disc_output')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------\n",
    "---------------------------------------------------------------------------\n",
    "\n",
    "## Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder-decoder architecture consists of:\n",
    "\n",
    "encoder:\n",
    "   \n",
    "    C64-C128-C256-C512-C512-C512-C512-C512\n",
    "decoder:\n",
    "\n",
    "    CD512-CD512-CD512-C512-C512-C256-C128-C64\n",
    "\n",
    "After the last layer in the decoder, a convolution is applied\n",
    "to map to the number of output channels (3 in general,\n",
    "except in colorization, where it is 2), followed by a Tanh\n",
    "function. As an exception to the above notation, BatchNorm\n",
    "is not applied to the first C64 layer in the encoder.\n",
    "All ReLUs in the encoder are leaky, with slope 0.2, while\n",
    "ReLUs in the decoder are not leaky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_block_unet(x, filters, bn, name):\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Conv2D(filters, (3, 3), strides=(2,2), name=name, padding=\"same\")(x)\n",
    "    if bn:\n",
    "        x = BatchNormalization()(x)\n",
    "    return x\n",
    "\n",
    "def deconv_block_unet(x, encoded, filters, bn, dropout, name):\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Conv2DTranspose(filters, (3, 3), strides=(2, 2), padding=\"same\")(x)\n",
    "    if bn:\n",
    "        x = BatchNormalization()(x)\n",
    "    if dropout:\n",
    "        x = Dropout(0.5)(x)\n",
    "    x = Concatenate()([x, encoded])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GENERATOR(img_dim=(256,256,3), nb_filters=64, output_dim=(256,256,2)):\n",
    "    input_layer = Input(shape=img_dim, name='unet_input')\n",
    "\n",
    "    # prepare encode filters\n",
    "    nb_conv = int(np.floor(np.log2(img_dim[1])))\n",
    "    list_filters = [nb_filters * min(8, (2 ** i)) for i in range(nb_conv)]\n",
    "\n",
    "    # encoder\n",
    "    list_encoder = []\n",
    "    list_encoder.append(Conv2D(list_filters[0], (3,3), \n",
    "                               strides=(2,2), name='unet_conv2D_1',\n",
    "                               padding='same')(input_layer))\n",
    "    for i, filters in enumerate(list_filters[1:]):\n",
    "        name = \"unet_conv2D_{}\".format(i + 2)\n",
    "        conv = conv_block_unet(list_encoder[-1], filters, True,name)\n",
    "        list_encoder.append(conv)\n",
    "\n",
    "    # Prepare decoder filters\n",
    "    list_filters = list_filters[:-1][::-1]\n",
    "    if len(list_filters) < nb_conv - 1:\n",
    "        list_filters.append(nb_filters)\n",
    "\n",
    "    # decoder\n",
    "    list_decoder = []\n",
    "    list_decoder.append(deconv_block_unet(list_encoder[-1], list_encoder[-2],\n",
    "                                        list_filters[0], bn=True, dropout=True,\n",
    "                                        name='unet_deconv2D_1'))\n",
    "    for i, filters in enumerate(list_filters[1:]):\n",
    "        name = 'unet_deconv2D_{}'.format(i+2)\n",
    "\n",
    "        if i < 2: d = True\n",
    "        else: d = False\n",
    "\n",
    "        conv = deconv_block_unet(list_decoder[-1], list_encoder[-(i+3)], filters,\n",
    "                                bn=True, dropout=d,name=name)\n",
    "        list_decoder.append(conv)\n",
    "\n",
    "    x = Activation('relu')(list_decoder[-1])\n",
    "    out_channels = output_dim[-1]\n",
    "    x = Conv2DTranspose(out_channels, (3,3), strides=(2,2), padding='same')(x)\n",
    "    x = Activation('tanh')(x)\n",
    "    return Model(inputs=input_layer, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------\n",
    "----------------------------------------------------------------------\n",
    "## Pic2Pic GAN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_patch(gen_input, gen_output, patch_size):\n",
    "    _, h, w, _ = gen_input.shape.as_list()\n",
    "    ph, pw = patch_size\n",
    "\n",
    "    list_row_idx = [(i * ph, (i + 1) * ph) for i in range(h // ph)]\n",
    "    list_col_idx = [(i * pw, (i + 1) * pw) for i in range(w // pw)]\n",
    "\n",
    "    list_gen_patch = []\n",
    "    for row_idx in list_row_idx:\n",
    "        for col_idx in list_col_idx:\n",
    "            x_patch = Lambda(lambda z: z[:, row_idx[0]:row_idx[1],\n",
    "                                         col_idx[0]:col_idx[1], :])(gen_output)\n",
    "            list_gen_patch.append(x_patch)\n",
    "    \n",
    "    return list_gen_patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PIC2PIC(generator, discriminator):\n",
    "    gen_input = generator.input\n",
    "    gen_output = generator.output\n",
    "    \n",
    "    # get the patch_size, patch_size is the input node size of discriminator\n",
    "    patch_node = discriminator.get_input_at(0)[0]\n",
    "    _, ph, pw, _ = patch_node.shape.as_list()\n",
    "    patch_size = (ph,pw)\n",
    "    \n",
    "    list_gen_patch = gen_patch(gen_input, gen_output, patch_size)        \n",
    "    patch_output = discriminator(list_gen_patch)\n",
    "\n",
    "    gan =  Model(inputs=[gen_input],\n",
    "                    outputs=[gen_output, patch_output],\n",
    "                    name='PIC2PIC_GAN')\n",
    "    return gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_dim = (256,256,3)\n",
    "patch_size = (64,64)\n",
    "nb_filters = 64\n",
    "output_dim = (256,256,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patch_dim = *patch_size, output_dim[-1]\n",
    "patchblock= PATCHBLOCK(patch_dim, img_dim, nb_filters)\n",
    "\n",
    "generator = GENERATOR(img_dim, nb_filters, output_dim)\n",
    "discriminator = DISCRIMINATOR(output_dim, patch_size, nb_filters)\n",
    "\n",
    "gan = PIC2PIC(generator, discriminator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
