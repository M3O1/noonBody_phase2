{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os, sys\n",
    "from datetime import datetime\n",
    "sys.path.append(\"../model/\")\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras.backend as K\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.utils import plot_model\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from unet import UNET\n",
    "\n",
    "from train_utils import extract_contour, fill_mask, to_uint8\n",
    "from train_utils import PlotCheckpoint, send_report_to_slack, set_directory, mean_iou, plot_sample_image, plot_bg_removal_sample_image\n",
    "from train_utils import get_disc_batch, np_mean_iou, to_uint8\n",
    "from data_utils import load_dataset, HumanSegGenerator\n",
    "from data_utils import clahe_func, gray_func\n",
    "from data_utils import rotation_func, rescaling_func, flip_func, random_crop_func, random_noise_func, gamma_func, color_gamma_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# TensorFlow wizardry\n",
    "config = tf.ConfigProto()\n",
    " \n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    " \n",
    "# Create a session with the above options specified.\n",
    "K.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First & Second Model\n",
    "=====================\n",
    "\n",
    "디폴트 실험. UNET을 기본으로 함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef_loss(y_true, y_pred):\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    union = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    return 1-(2. * intersection + 1e-5) / (union + 1e-5)\n",
    "    \n",
    "def bcf(y_true, y_pred):\n",
    "    return K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1)\n",
    "\n",
    "def bcf_dice_loss(loss_d):\n",
    "    def merge_loss(y_true, y_pred):\n",
    "        dice_loss = dice_coef_loss(y_true, y_pred)\n",
    "        bcf_loss = bcf(y_true, y_pred)\n",
    "        return loss_d * dice_loss + (1. - loss_d) * bcf_loss\n",
    "    return merge_loss\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    union = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    return (2. * intersection + 1e-5) / (union + 1e-5)\n",
    "\n",
    "def jaccard_distance(y_true, y_pred, smooth=100):\n",
    "    \"\"\"Jaccard distance for semantic segmentation, also known as the intersection-over-union loss.\n",
    "    This loss is useful when you have unbalanced numbers of pixels within an image\n",
    "    because it gives all classes equal weight. However, it is not the defacto\n",
    "    standard for image segmentation.\n",
    "    For example, assume you are trying to predict if each pixel is cat, dog, or background.\n",
    "    You have 80% background pixels, 10% dog, and 10% cat. If the model predicts 100% background\n",
    "    should it be be 80% right (as with categorical cross entropy) or 30% (with this loss)?\n",
    "    The loss has been modified to have a smooth gradient as it converges on zero.\n",
    "    This has been shifted so it converges on 0 and is smoothed to avoid exploding\n",
    "    or disappearing gradient.\n",
    "    Jaccard = (|X & Y|)/ (|X|+ |Y| - |X & Y|)\n",
    "            = sum(|A*B|)/(sum(|A|)+sum(|B|)-sum(|A*B|))\n",
    "    # References\n",
    "    Csurka, Gabriela & Larlus, Diane & Perronnin, Florent. (2013).\n",
    "    What is a good evaluation measure for semantic segmentation?.\n",
    "    IEEE Trans. Pattern Anal. Mach. Intell.. 26. . 10.5244/C.27.32.\n",
    "    https://en.wikipedia.org/wiki/Jaccard_index\n",
    "    \"\"\"\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return (1 - jac) * smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"unet\"\n",
    "dataset_name = 'train_refined'\n",
    "h5_path = \"../data/baidu-segmentation.h5\"\n",
    "\n",
    "batch_size = 1\n",
    "if batch_size == 1:\n",
    "    instance = True\n",
    "else:\n",
    "    instance = False\n",
    "    \n",
    "nb_epoch = 200\n",
    "\n",
    "img_dim = (256,256,3)\n",
    "output_dim = (256,256,1)\n",
    "\n",
    "nb_filter = 16\n",
    "depth = 4\n",
    "activation= 'LeakyReLU'\n",
    "bn = True\n",
    "lr = 0.0002\n",
    "beta_1 = 0.5\n",
    "loss_function = 'binary_crossentropy'\n",
    "sigmoid = True\n",
    "drop_rate = 0.\n",
    "bg_removal = False\n",
    "\n",
    "title = \"17th trial - loss Change\"\n",
    "description = '''\n",
    "Binary Cross Entropy\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data generator\n",
    "trainset = load_dataset(dataset_name=dataset_name,h5_path=h5_path)\n",
    "\n",
    "epoch_size = len(trainset)\n",
    "train_steps = epoch_size // batch_size\n",
    "\n",
    "traingen = HumanSegGenerator(trainset,img_dim,\n",
    "                             batch_size=batch_size,\n",
    "                             sigmoid=sigmoid,\n",
    "                             bg_removal=bg_removal,\n",
    "                             aug_funcs=[rotation_func(), \n",
    "                                        rescaling_func(),\n",
    "                                        flip_func(), \n",
    "                                        random_crop_func(),\n",
    "                                        random_noise_func(),\n",
    "                                        gamma_func(),\n",
    "                                        color_gamma_func()],\n",
    "                             prep_funcs=[])\n",
    "\n",
    "testset = load_dataset(dataset_name='test',h5_path=h5_path)\n",
    "test_images, test_profiles = next(HumanSegGenerator(testset,img_dim,\n",
    "                                                    batch_size=len(testset),\n",
    "                                                    sigmoid=sigmoid,\n",
    "                                                    bg_removal=bg_removal,\n",
    "                                                    prep_funcs=[]))\n",
    "\n",
    "# create the model\n",
    "unet = UNET(img_dim=img_dim,\n",
    "            depth=depth,\n",
    "            nb_filter=nb_filter,\n",
    "            output_dim=output_dim,\n",
    "            bn=bn,\n",
    "            instance=instance,\n",
    "            sigmoid=sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the model directory to save \n",
    "weights_dir, model_dir, sample_dir = set_directory(\"../results\",model_name)\n",
    "\n",
    "# plot the model arch image\n",
    "model_path = os.path.join(model_dir, \"unet.png\")\n",
    "plot_model(unet, to_file=model_path, show_shapes=True)\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = unet.to_json()\n",
    "with open(os.path.join(model_dir,\"model.json\"), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# set callback function\n",
    "tqdm = TQDMNotebookCallback()\n",
    "modelcheckpoint = ModelCheckpoint(os.path.join(weights_dir,\n",
    "                                               \"model_{val_mean_iou:.2f}.h5\"),\n",
    "                                  monitor='val_mean_iou',\n",
    "                                  mode='max',\n",
    "                                  save_weights_only=True,\n",
    "                                  save_best_only=True,\n",
    "                                  period=1)\n",
    "earlystopping = EarlyStopping(monitor='val_loss',\n",
    "                              mode='min',\n",
    "                              patience=10)\n",
    "plotcheckpoint = PlotCheckpoint(unet,test_images, sample_dir)\n",
    "callbacks= [tqdm, modelcheckpoint, earlystopping, plotcheckpoint]\n",
    "\n",
    "# set loss and optimizer\n",
    "unet.compile(Adam(lr=lr,beta_1=beta_1),\n",
    "            loss=loss_function,\n",
    "            metrics=[mean_iou, dice_coef])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist = unet.fit_generator(traingen,\n",
    "                           train_steps,\n",
    "                           epochs=nb_epoch,\n",
    "                           verbose=0, \n",
    "                           validation_data = (test_images, test_profiles),\n",
    "                           callbacks=callbacks)\n",
    "\n",
    "# Draw the results\n",
    "df = pd.DataFrame({\"train-loss\":hist.history['loss'],\n",
    "                   \"val-loss\":hist.history['val_loss'],\n",
    "                   \"train-mean_iou\":hist.history['mean_iou'],\n",
    "                   \"val-mean_iou\":hist.history['val_mean_iou']})\n",
    "df.to_csv(os.path.join(model_dir,\"training.csv\"))\n",
    "df.plot().figure.savefig(os.path.join(model_dir,\"training.png\"))\n",
    "\n",
    "# serialize weights to HDF5\n",
    "unet.save_weights(os.path.join(weights_dir,\"model.h5\"))\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "# Send to results\n",
    "best_idx = df['val-mean_iou'].sort_values(ascending=False).index[0]\n",
    "best_iou = df['val-mean_iou'].sort_values(ascending=False).iloc[0]\n",
    "\n",
    "description = description \\\n",
    "+ \"\\nmodel_path : {}\".format(weights_dir[:-7])\\\n",
    "+ \"\\nbest_mean_iou : {}\".format(best_iou)\n",
    "\n",
    "image_paths = {\n",
    "    'model' : os.path.join(model_dir,\"unet.png\"),\n",
    "    'train-process' : os.path.join(model_dir,'training.png'),\n",
    "    'train-process-csv' : os.path.join(model_dir,\"training.csv\"),\n",
    "    'test image' : os.path.join(sample_dir,\"{:02d}_epoch_sample.png\".format(best_idx))\n",
    "}\n",
    "send_report_to_slack(title,description,image_paths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
