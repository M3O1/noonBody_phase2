{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os, sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "sys.path.append(\"../model/\")\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import keras.backend as K\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.utils import plot_model, generic_utils\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "from unet import UNET\n",
    "from vgan import VGAN, DISCRIMINATOR, PIC_DISCRIMINATOR\n",
    "\n",
    "from train_utils import extract_contour, fill_mask, to_uint8\n",
    "from train_utils import PlotCheckpoint, send_report_to_slack, set_directory, mean_iou, plot_sample_image, plot_bg_removal_sample_image\n",
    "from train_utils import get_disc_batch, np_mean_iou, to_uint8\n",
    "from data_utils import load_dataset, HumanSegGenerator\n",
    "from data_utils import clahe_func, gray_func\n",
    "from data_utils import rotation_func, rescaling_func, flip_func, random_crop_func, random_noise_func, gamma_func, color_gamma_func\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_mean_iou(y_pred, y_true, thr=0.5):\n",
    "    y_pred = (y_pred>thr).astype(np.int)\n",
    "    y_true = (y_true>thr).astype(np.int)\n",
    "\n",
    "    intersect = y_pred * y_true\n",
    "    union = np.ones_like(y_pred) - ((1-y_pred) * (1-y_true))\n",
    "    return np.mean(np.sum(np.sum(intersect,axis=1),axis=1) / \\\n",
    "                   np.sum(np.sum(union,axis=1),axis=1))\n",
    "\n",
    "def to_mask(image_batch,thr=-.99):\n",
    "    images = []\n",
    "    for image in image_batch:\n",
    "        mask = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        mask = (mask > thr).astype(np.float32)\n",
    "        images.append(mask)\n",
    "    return np.stack(images, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gan\"\n",
    "dataset_name = 'train_refined'\n",
    "h5_path = \"../data/baidu-segmentation.h5\"\n",
    "\n",
    "# train setting\n",
    "batch_size = 1\n",
    "if batch_size == 1:\n",
    "    instance = True\n",
    "else:\n",
    "    instance = False\n",
    "\n",
    "nb_epoch = 200\n",
    "\n",
    "# Model Setting\n",
    "img_dim = (256,256,3)\n",
    "output_dim = (256,256,1)\n",
    "activation = 'LeakyReLU'\n",
    "bn = True\n",
    "bg_removal = False\n",
    "sigmoid = False\n",
    "\n",
    "if sigmoid:\n",
    "    # 범위가 0~1이므로 threshold는 0.5\n",
    "    thr = 0.5\n",
    "else:\n",
    "    # 범위가 -1~1이므로 threshold는 0.\n",
    "    thr = 0.\n",
    "\n",
    "# Generator Setting\n",
    "nb_filter = 16\n",
    "depth = 4\n",
    "lr = 0.0002\n",
    "beta_1 = 0.5\n",
    "loss_function = \"mae\"\n",
    "#loss_function = 'binary_crossentropy'\n",
    "drop_rate = 0.\n",
    "\n",
    "# Discriminator Setting\n",
    "nb_filter_disc = 32\n",
    "lr_disc = 0.0001\n",
    "beta_1_disc = 0.5\n",
    "depth_disc = 6\n",
    "loss_function_disc = \"binary_crossentropy\"\n",
    "opt_type = 'adam' # opt_type = 'sgd'\n",
    "drop_rate = 0.\n",
    "\n",
    "# gan Setting\n",
    "alpha = 10\n",
    "\n",
    "# training Setting\n",
    "label_smoothing = False\n",
    "label_flipping = 0.0\n",
    "round_size = 1000\n",
    "\n",
    "\n",
    "load = False\n",
    "load_gen_path = \"../results/gan/0706_06/model-arch/generator.json\"\n",
    "load_gen_weight_path = \"../results/gan/0706_06/weights/0.883_gen.h5\"\n",
    "load_disc_path = \"../results/gan/0706_06/model-arch/discriminator.json\"\n",
    "load_disc_weight_path = \"../results/gan/0706_06/weights/0.883_disc.h5\"\n",
    "\n",
    "title = \"6th trial - Change Loss function\"\n",
    "description = '''\n",
    "    generator output은 sigmoid -> tanh으로!\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo-List\n",
    "\n",
    "    1. Dropout을 추가하였을 때, 어떤 식으로 동작하는지 알아보기. \n",
    "    2. Discriminator의 학습 바꾸었을 때 어떤식으로 동작하는 지 확인\n",
    "    3. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load:\n",
    "    with open(load_disc_path) as f:\n",
    "        discriminator = model_from_json(f.readline())\n",
    "        discriminator.load_weights(load_disc_weight_path)\n",
    "    with open(load_gen_path) as f:\n",
    "        generator = model_from_json(f.readline())\n",
    "        generator.load_weights(load_gen_weight_path)\n",
    "    ## gan\n",
    "    gan = VGAN(generator,discriminator)\n",
    "else:    \n",
    "    # create the model\n",
    "    ## generator\n",
    "    generator = UNET(img_dim=img_dim,\n",
    "                depth=depth,\n",
    "                nb_filter=nb_filter,\n",
    "                output_dim=output_dim,\n",
    "                bn=bn,\n",
    "                instance=instance,\n",
    "                activation=activation,\n",
    "                sigmoid=sigmoid)\n",
    "\n",
    "    ## discriminator\n",
    "    discriminator = PIC_DISCRIMINATOR(img_dim=img_dim,\n",
    "                        nb_filter=nb_filter_disc,\n",
    "                        depth=depth_disc,\n",
    "                        activation=activation,\n",
    "                        bn=bn,\n",
    "                        instance=instance,\n",
    "                        output_dim=output_dim)\n",
    "    ## gan\n",
    "    gan = VGAN(generator,discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set the optimizer\n",
    "opt_generator = Adam(lr=lr, beta_1=beta_1, beta_2=0.999)\n",
    "if opt_type == 'sgd':\n",
    "    opt_discriminator = SGD(lr=lr_disc, momentum=0.9, nesterov=True)\n",
    "else:\n",
    "    opt_discriminator = Adam(lr=lr_disc, beta_1=beta_1, beta_2=0.999)\n",
    "\n",
    "## set the loss function\n",
    "discriminator.trainable = False\n",
    "generator.compile(loss=loss_function, optimizer=opt_generator) \n",
    "\n",
    "loss = [loss_function, loss_function_disc]\n",
    "loss_weights = [alpha, 1]\n",
    "gan.compile(loss=loss_function, loss_weights=loss_weights, optimizer=opt_generator)\n",
    "\n",
    "discriminator.trainable = True\n",
    "discriminator.compile(loss=loss_function_disc, optimizer=opt_discriminator)\n",
    "\n",
    "# create data generator\n",
    "trainset = load_dataset(dataset_name=dataset_name,h5_path=h5_path)\n",
    "\n",
    "epoch_size = len(trainset)\n",
    "train_steps = epoch_size // batch_size\n",
    "\n",
    "traingen = HumanSegGenerator(trainset,img_dim,\n",
    "                             batch_size=batch_size,\n",
    "                             sigmoid=sigmoid,\n",
    "                             bg_removal=bg_removal,\n",
    "                             aug_funcs=[rotation_func(), \n",
    "                                        rescaling_func(),\n",
    "                                        flip_func(), \n",
    "                                        random_crop_func(),\n",
    "                                        random_noise_func(),\n",
    "                                        gamma_func(),\n",
    "                                        color_gamma_func()],\n",
    "                             prep_funcs=[])\n",
    "\n",
    "testset = load_dataset(dataset_name='test',h5_path=h5_path)\n",
    "test_images, test_profiles = next(HumanSegGenerator(testset,img_dim,\n",
    "                                                    batch_size=len(testset),\n",
    "                                                    sigmoid=sigmoid,\n",
    "                                                    bg_removal=bg_removal,\n",
    "                                                    prep_funcs=[]))\n",
    "\n",
    "loss_df =  pd.DataFrame()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# set the model directory to save \n",
    "weights_dir, model_dir, sample_dir = set_directory(\"../results\",model_name)\n",
    "\n",
    "# draw the architecture diagram of model\n",
    "model_path = os.path.join(model_dir, \"generator.png\")\n",
    "plot_model(generator, to_file=model_path, show_shapes=True)\n",
    "\n",
    "model_path = os.path.join(model_dir, \"discriminator.png\")\n",
    "plot_model(discriminator, to_file=model_path, show_shapes=True)\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = generator.to_json()\n",
    "with open(os.path.join(model_dir,\"generator.json\"), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "model_json = discriminator.to_json()\n",
    "with open(os.path.join(model_dir,\"discriminator.json\"), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# train the Model\n",
    "step = 0\n",
    "best_iou = 0\n",
    "for e in range(nb_epoch):\n",
    "    progbar = generic_utils.Progbar(epoch_size)\n",
    "    disc_losses = []; g_tot_losses = []\n",
    "    g_losses = []; g_log_losses = []\n",
    "    \n",
    "    for _ in range(train_steps):\n",
    "        step += 1\n",
    "        image_batch, profile_batch = next(traingen)\n",
    "        # Create a batch to feed the discriminator model\n",
    "        X_disc, y_disc = get_disc_batch(image_batch, profile_batch, \n",
    "                                        generator, step,\n",
    "                                        label_smoothing=label_smoothing,\n",
    "                                        label_flipping=label_flipping)\n",
    "        \n",
    "        # Update the discriminator\n",
    "        disc_loss = discriminator.train_on_batch(X_disc, y_disc)            \n",
    "        \n",
    "        # Create a batch to feed the generator model\n",
    "        y_gen = np.zeros((image_batch.shape[0], 2))\n",
    "        y_gen[:, 1] = 1.\n",
    "        \n",
    "        # Freeze the discriminator\n",
    "        discriminator.trainable = False\n",
    "        gen_loss = gan.train_on_batch(image_batch, [profile_batch, y_gen])\n",
    "        # Unfreeze the discriminator\n",
    "        discriminator.trainable = True\n",
    "        \n",
    "        # Calculate IOU\n",
    "        progbar.add(batch_size, values=[(\"D logloss\", disc_loss), (\"G tot\", gen_loss[0]),\n",
    "                                        (\"G {}\".format(loss_function), gen_loss[1]),\n",
    "                                        (\"G logloss\", gen_loss[2])])\n",
    "        \n",
    "        disc_losses.append(disc_loss); g_tot_losses.append(gen_loss[0])\n",
    "        g_losses.append(gen_loss[1]); g_log_losses.append(gen_loss[2])\n",
    "        \n",
    "        # save the training progress\n",
    "        if step % (train_steps // 4) == 0:\n",
    "            gen_batch = generator.predict_on_batch(image_batch)\n",
    "            iou = np_mean_iou(gen_batch, profile_batch, thr)\n",
    "            gen_test_batch = generator.predict_on_batch(test_images)\n",
    "            test_iou = np_mean_iou(gen_test_batch, test_profiles, thr)\n",
    "            \n",
    "            loss_df = loss_df.append({\n",
    "                    \"step\" : step, \"epoch\" : e,\n",
    "                    \"train D logloss\" : np.mean(disc_losses),\n",
    "                    \"train G tot\" : np.mean(g_tot_losses),\n",
    "                    \"train G {}\".format(loss_function) : np.mean(g_losses),\n",
    "                    \"train G logloss\": np.mean(g_log_losses),\n",
    "                    \"train IOU\": iou, \"test IOU\": test_iou\n",
    "                },ignore_index=True)\n",
    "            if test_iou > best_iou:\n",
    "                generator.save_weights(os.path.join(weights_dir,\"{:1.3f}_gen.h5\".format(best_iou)))\n",
    "                discriminator.save_weights(os.path.join(weights_dir,\"{:1.3f}_disc.h5\".format(best_iou)))\n",
    "                gan.save_weights(os.path.join(weights_dir,\"{:1.3f}_gan.h5\".format(best_iou)))\n",
    "                best_iou = test_iou\n",
    "                \n",
    "            disc_losses = []; g_tot_losses = []\n",
    "            g_losses = []; g_log_losses = []\n",
    "            print(\"\\n{:03d}_epoch test IOU : {:.4f}\".format(e, test_iou))\n",
    "    \n",
    "    # Draw the Result\n",
    "    filename = \"{:03d}_epoch_sample.png\".format(e)\n",
    "    sample_path = os.path.join(sample_dir,filename)            \n",
    "    gen_profiles = generator.predict_on_batch(test_images)\n",
    "    if bg_removal:\n",
    "        plot_bg_removal_sample_image(test_images, gen_profiles, sample_path)\n",
    "    else:\n",
    "        plot_sample_image(test_images, gen_profiles, sample_path)\n",
    "        \n",
    "    # serialize weights to HDF5\n",
    "    best_idx = loss_df[['test IOU','epoch']].sort_values('test IOU',ascending=False).iloc[0]['epoch']\n",
    "    best_iou = loss_df['test IOU'].sort_values(ascending=False).iloc[0]\n",
    "    generator.save_weights(os.path.join(weights_dir,\"{:1.3f}_gen.h5\".format(best_iou)))\n",
    "    discriminator.save_weights(os.path.join(weights_dir,\"{:1.3f}_disc.h5\".format(best_iou)))\n",
    "    gan.save_weights(os.path.join(weights_dir,\"{:1.3f}_gan.h5\".format(best_iou)))\n",
    "    \n",
    "    # Draw the Training Learning Curve\n",
    "    loss_df = loss_df.round(4)\n",
    "    loss_df.to_csv(os.path.join(model_dir, \"training.csv\"),index=False)\n",
    "    loss_df[['train IOU','test IOU']].plot().figure.savefig(os.path.join(model_dir,\"iou_training.png\"))\n",
    "    loss_df[['train D logloss','train G tot']].plot().figure.savefig(os.path.join(model_dir,\"loss_training.png\"))\n",
    "    loss_df[['train G logloss','train G {}'.format(loss_function)]].plot().figure.savefig(os.path.join(model_dir,\"gen_loss_training.png\"))\n",
    "    \n",
    "description = description \\\n",
    "+ \"\\nmodel_path : {}\".format(weights_dir[:-7])\\\n",
    "+ \"\\nbest_mean_iou : {}\".format(best_iou)\n",
    "                              \n",
    "image_paths = {\n",
    "    'gen_model' : os.path.join(model_dir,\"generator.png\"),\n",
    "    \"disc_model\" : os.path.join(model_dir,\"discriminator.png\"),\n",
    "    'iou-training' : os.path.join(model_dir,'iou_training.png'),\n",
    "    'loss-training' : os.path.join(model_dir,'loss_training.png'), \n",
    "    'train-process-csv' : os.path.join(model_dir,\"training.csv\"),\n",
    "    'test image' : os.path.join(sample_dir,\"{:03d}_epoch_sample.png\".format(int(best_idx)))\n",
    "}\n",
    "\n",
    "send_report_to_slack(title,description,image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the model directory to save \n",
    "weights_dir, model_dir, sample_dir = set_directory(\"../results\",model_name)\n",
    "\n",
    "# draw the architecture diagram of model\n",
    "model_path = os.path.join(model_dir, \"generator.png\")\n",
    "plot_model(generator, to_file=model_path, show_shapes=True)\n",
    "\n",
    "model_path = os.path.join(model_dir, \"discriminator.png\")\n",
    "plot_model(discriminator, to_file=model_path, show_shapes=True)\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = generator.to_json()\n",
    "with open(os.path.join(model_dir,\"generator.json\"), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "model_json = discriminator.to_json()\n",
    "with open(os.path.join(model_dir,\"discriminator.json\"), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# train the Model\n",
    "best_iou = 0\n",
    "for e in range(nb_epoch):\n",
    "    progbar = generic_utils.Progbar(epoch_size)\n",
    "    disc_losses = []; g_tot_losses = []\n",
    "    g_losses = []; g_log_losses = []\n",
    "    \n",
    "    for _ in range(train_steps//(round_size*2)):\n",
    "        # Update the discriminator\n",
    "        # Unfreeze the discriminator\n",
    "        discriminator.trainable = True\n",
    "        for i in range(round_size):\n",
    "            image_batch, profile_batch = next(traingen)\n",
    "            X_disc, y_disc = get_disc_batch(image_batch, profile_batch, \n",
    "                                            generator, i,\n",
    "                                            label_smoothing=label_smoothing,\n",
    "                                            label_flipping=label_flipping)\n",
    "            disc_loss = discriminator.train_on_batch(X_disc, y_disc)\n",
    "            disc_losses.append(disc_loss);\n",
    "            \n",
    "            progbar.add(batch_size, values=[(\"D logloss\", disc_loss)])\n",
    "            \n",
    "        \n",
    "        # update the Generator\n",
    "        # Freeze the discriminator\n",
    "        discriminator.trainable = False\n",
    "        for i in range(round_size):\n",
    "            image_batch, profile_batch = next(traingen)\n",
    "\n",
    "            y_gen = np.zeros((image_batch.shape[0], 2))\n",
    "            y_gen[:, 1] = 1.\n",
    "            gen_loss = gan.train_on_batch(image_batch, [profile_batch, y_gen])\n",
    "            \n",
    "            g_tot_losses.append(gen_loss[0])\n",
    "            g_losses.append(gen_loss[1])\n",
    "            g_log_losses.append(gen_loss[2])\n",
    "\n",
    "            progbar.add(batch_size,values=[(\"G tot\", gen_loss[0]),\n",
    "                                   (\"G {}\".format(loss_function), gen_loss[1]),\n",
    "                                    (\"G logloss\", gen_loss[2])])\n",
    "            \n",
    "        # save the training progress\n",
    "        gen_batch = generator.predict_on_batch(image_batch)\n",
    "        iou = np_mean_iou(gen_batch, profile_batch, thr)\n",
    "        gen_test_batch = generator.predict_on_batch(test_images)\n",
    "        test_iou = np_mean_iou(gen_test_batch, test_profiles, thr)\n",
    "        \n",
    "        loss_df = loss_df.append({\n",
    "                \"epoch\" : e,\n",
    "                \"train D logloss\" : np.mean(disc_losses),\n",
    "                \"train G tot\" : np.mean(g_tot_losses),\n",
    "                \"train G {}\".format(loss_function) : np.mean(g_losses),\n",
    "                \"train G logloss\": np.mean(g_log_losses),\n",
    "                \"train IOU\": iou, \"test IOU\": test_iou\n",
    "            },ignore_index=True)\n",
    "                \n",
    "        if test_iou > best_iou:\n",
    "            generator.save_weights(os.path.join(weights_dir,\"{:1.3f}_gen.h5\".format(best_iou)))\n",
    "            discriminator.save_weights(os.path.join(weights_dir,\"{:1.3f}_disc.h5\".format(best_iou)))\n",
    "            gan.save_weights(os.path.join(weights_dir,\"{:1.3f}_gan.h5\".format(best_iou)))\n",
    "            best_iou = test_iou\n",
    "\n",
    "        disc_losses = []; g_tot_losses = []\n",
    "        g_losses = []; g_log_losses = []\n",
    "        print(\"\\n{:03d}_epoch test IOU : {:.4f}\".format(e, test_iou))\n",
    "    \n",
    "    # Draw the Result\n",
    "    filename = \"{:03d}_epoch_sample.png\".format(e)\n",
    "    sample_path = os.path.join(sample_dir,filename)            \n",
    "    gen_profiles = generator.predict_on_batch(test_images)\n",
    "    if bg_removal:\n",
    "        plot_bg_removal_sample_image(test_images, gen_profiles, sample_path)\n",
    "    else:\n",
    "        plot_sample_image(test_images, gen_profiles, sample_path)\n",
    "        \n",
    "    # serialize weights to HDF5\n",
    "    best_idx = loss_df[['test IOU','epoch']].sort_values('test IOU',ascending=False).iloc[0]['epoch']\n",
    "    best_iou = loss_df['test IOU'].sort_values(ascending=False).iloc[0]\n",
    "    generator.save_weights(os.path.join(weights_dir,\"{:1.3f}_gen.h5\".format(best_iou)))\n",
    "    discriminator.save_weights(os.path.join(weights_dir,\"{:1.3f}_disc.h5\".format(best_iou)))\n",
    "    gan.save_weights(os.path.join(weights_dir,\"{:1.3f}_gan.h5\".format(best_iou)))\n",
    "    \n",
    "    # Draw the Training Learning Curve\n",
    "    loss_df = loss_df.round(4)\n",
    "    loss_df.to_csv(os.path.join(model_dir, \"training.csv\"),index=False)\n",
    "    loss_df[['train IOU','test IOU']].plot().figure.savefig(os.path.join(model_dir,\"iou_training.png\"))\n",
    "    loss_df[['train D logloss','train G tot']].plot().figure.savefig(os.path.join(model_dir,\"loss_training.png\"))\n",
    "    loss_df[['train G logloss','train G {}'.format(loss_function)]].plot().figure.savefig(os.path.join(model_dir,\"gen_loss_training.png\"))\n",
    "    \n",
    "description = description \\\n",
    "+ \"\\nmodel_path : {}\".format(weights_dir[:-7])\\\n",
    "+ \"\\nbest_mean_iou : {}\".format(best_iou)\n",
    "                              \n",
    "image_paths = {\n",
    "    'gen_model' : os.path.join(model_dir,\"generator.png\"),\n",
    "    \"disc_model\" : os.path.join(model_dir,\"discriminator.png\"),\n",
    "    'iou-training' : os.path.join(model_dir,'iou_training.png'),\n",
    "    'loss-training' : os.path.join(model_dir,'loss_training.png'), \n",
    "    'train-process-csv' : os.path.join(model_dir,\"training.csv\"),\n",
    "    'test image' : os.path.join(sample_dir,\"{:03d}_epoch_sample.png\".format(int(best_idx)))\n",
    "}\n",
    "\n",
    "send_report_to_slack(title,description,image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
