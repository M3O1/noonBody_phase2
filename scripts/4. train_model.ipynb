{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ksj/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os, sys\n",
    "import time\n",
    "sys.path.append(\"../model/\")\n",
    "\n",
    "from model import PIC2PIC, DISCRIMINATOR, GENERATOR, PATCHBLOCK\n",
    "from utils import HumanSegGenerator, load_dataset, gen_sample, plot_generated_batch, get_disc_batch\n",
    "\n",
    "from keras.utils import generic_utils\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.utils import plot_model\n",
    "import keras.backend as K\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://github.com/ChengBinJin/V-GAN-tensorflow/blob/master/codes/model.py\n",
    "2. https://github.com/tdeboissiere/DeepLearningImplementations/blob/master/pix2pix/src/model/train.py\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training details\n",
    "\n",
    "Random jitter was applied by resizing the 256×256 input\n",
    "images to 286 × 286 and then randomly cropping back to\n",
    "size 256 × 256.\n",
    "\n",
    "All networks were trained from scratch. Weights were\n",
    "initialized from a Gaussian distribution with mean 0 and\n",
    "standard deviation 0.02.\n",
    "\n",
    "Maps↔aerial photograph 1096 training images\n",
    "scraped from Google Maps, trained for 200 epochs, batch\n",
    "size 1, with random jitter and mirroring. Images were\n",
    "sampled from in and around New York City. Data was then\n",
    "split into train and test about the median latitude of the\n",
    "sampling region (with a buffer region added to ensure that\n",
    "no training pixel appeared in the test set).\n",
    "\n",
    "We use minibatch SGD and\n",
    "apply the Adam solver [31], with learning rate 0.0002, and\n",
    "momentum parameters β 1 = 0.5, β 2 = 0.999.\n",
    "\n",
    "In addition, we divide the objective by 2 while optimizing D, which slows down the rate at\n",
    "which D learns relative to G. We use minibatch SGD and\n",
    "apply the Adam solver [31], with learning rate 0.0002, and\n",
    "momentum parameters β 1 = 0.5, β 2 = 0.999."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = \"pix2pix\"\n",
    "\n",
    "batch_size = 1\n",
    "nb_epoch = 70 # 1096 * 200 / 3500\n",
    "\n",
    "img_dim = (256,256,3)\n",
    "patch_size = (64,64)\n",
    "nb_filter = 64\n",
    "output_dim = (256,256,1)\n",
    "\n",
    "nb_sample = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the model directory to save \n",
    "result_dir = \"../results/{}/\".format(model_name)\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "# save weights\n",
    "weights_dir = os.path.join(result_dir,'weights')\n",
    "os.makedirs(weights_dir,exist_ok=True)\n",
    "# save model diagram \n",
    "model_dir = os.path.join(result_dir, \"model-arch\")\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "# save sample iamge\n",
    "sample_dir = os.path.join(result_dir,\"sample\")\n",
    "os.makedirs(sample_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the model\n",
    "patch_dim = *patch_size, output_dim[-1]\n",
    "patchblock= PATCHBLOCK(patch_dim, img_dim, nb_filter)\n",
    "\n",
    "generator = GENERATOR(img_dim, 16, output_dim)\n",
    "discriminator = DISCRIMINATOR(output_dim, patch_size, nb_filter)\n",
    "\n",
    "gan = PIC2PIC(generator, discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# draw the architecture diagram of model\n",
    "model_path = os.path.join(model_dir, \"patchblock.png\")\n",
    "plot_model(patchblock, to_file=model_path, show_shapes=True)\n",
    "\n",
    "model_path = os.path.join(model_dir, \"generator.png\")\n",
    "plot_model(generator, to_file=model_path, show_shapes=True)\n",
    "\n",
    "model_path = os.path.join(model_dir, \"discriminator.png\")\n",
    "plot_model(discriminator, to_file=model_path, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the optimizer\n",
    "opt_generator = Adam(lr=2E-4, beta_1=0.5, beta_2=0.999)\n",
    "opt_discriminator = Adam(lr=1E-4, beta_1=0.5, beta_2=0.999)\n",
    "\n",
    "# set the loss function\n",
    "discriminator.trainable = False\n",
    "generator.compile(loss='mae', optimizer=opt_discriminator) # legacy for save_weights\n",
    "\n",
    "loss = ['mean_absolute_error', 'binary_crossentropy']\n",
    "loss_weights = [1e1, 1]\n",
    "gan.compile(loss=loss, loss_weights=loss_weights, optimizer=opt_generator)\n",
    "\n",
    "discriminator.trainable = True\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=opt_discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the train and test dataset\n",
    "train, test = train_test_split(load_dataset(), test_size=409)\n",
    "\n",
    "train_generator = HumanSegGenerator(train, img_dim, batch_size=1)\n",
    "test_generator = HumanSegGenerator(test, img_dim, batch_size=1, is_train=True)\n",
    "\n",
    "epoch_size = train.shape[0]\n",
    "n_batch_per_epoch = epoch_size // batch_size\n",
    "\n",
    "loss_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid dimensions for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-0b9721981a6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minverse_normalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_disc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, hold, data, **kwargs)\u001b[0m\n\u001b[1;32m   3078\u001b[0m                         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3079\u001b[0m                         \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                         **kwargs)\n\u001b[0m\u001b[1;32m   3081\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3082\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1708\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1709\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1710\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1711\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1712\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5192\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5194\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5195\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    602\u001b[0m         if not (self._A.ndim == 2\n\u001b[1;32m    603\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[0;32m--> 604\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid dimensions for image data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_imcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid dimensions for image data"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAAD8CAYAAACVSwr3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAADJlJREFUeJzt22GI5Hd9x/H3x1xTaRq1mBXk7jSR\nXqrXUIhd0hShRkzLJYW7JyJ3EFpL8NAa+0AppFhSiY8aaQXhWnu0EhU0nj6oi5wEtBGLeJoN0ehd\nuLI9bbNEmlPTPBGNod8+mNFO5rt7+7/L7Mwtfb9gYf7/+c3sd4e59/7nv/9LVSFJk1606AEkXX4M\ng6TGMEhqDIOkxjBIagyDpGbLMCT5aJKnknxnk/uT5MNJ1pI8luT1sx9T0jwNOWK4HzhwgftvA/aN\nv44Cf//Cx5K0SFuGoaq+AvzoAksOAR+vkVPAy5K8clYDSpq/XTN4jt3AExPb6+N9359emOQoo6MK\nrrrqqt9+7WtfO4NvL2kzjzzyyA+qauliHzeLMGSDfRteZ11Vx4HjAMvLy7W6ujqDby9pM0n+41Ie\nN4u/SqwDeye29wBPzuB5JS3ILMKwAvzR+K8TNwPPVFX7GCFp59jyo0SSTwG3ANckWQf+CvglgKr6\nCHASuB1YA34M/Ml2DStpPrYMQ1Ud2eL+At41s4kkLZxXPkpqDIOkxjBIagyDpMYwSGoMg6TGMEhq\nDIOkxjBIagyDpMYwSGoMg6TGMEhqDIOkxjBIagyDpMYwSGoMg6TGMEhqDIOkxjBIagyDpMYwSGoM\ng6TGMEhqDIOkxjBIagyDpMYwSGoMg6TGMEhqDIOkxjBIagyDpMYwSGoGhSHJgSRnk6wluXuD+1+V\n5KEkjyZ5LMntsx9V0rxsGYYkVwDHgNuA/cCRJPunlv0lcKKqbgQOA38360Elzc+QI4abgLWqOldV\nzwIPAIem1hTwkvHtlwJPzm5ESfM2JAy7gScmttfH+ya9H7gjyTpwEnj3Rk+U5GiS1SSr58+fv4Rx\nJc3DkDBkg301tX0EuL+q9gC3A59I0p67qo5X1XJVLS8tLV38tJLmYkgY1oG9E9t76B8V7gROAFTV\n14AXA9fMYkBJ8zckDA8D+5Jcl+RKRicXV6bW/CfwZoAkr2MUBj8rSDvUlmGoqueAu4AHgccZ/fXh\ndJJ7kxwcL3sv8PYk3wI+BbytqqY/bkjaIXYNWVRVJxmdVJzcd8/E7TPAG2Y7mqRF8cpHSY1hkNQY\nBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgG\nSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBkmNYZDUDApD\nkgNJziZZS3L3JmvemuRMktNJPjnbMSXN066tFiS5AjgG/D6wDjycZKWqzkys2Qf8BfCGqno6ySu2\na2BJ22/IEcNNwFpVnauqZ4EHgENTa94OHKuqpwGq6qnZjilpnoaEYTfwxMT2+njfpOuB65N8Ncmp\nJAc2eqIkR5OsJlk9f/78pU0sadsNCUM22FdT27uAfcAtwBHgH5O8rD2o6nhVLVfV8tLS0sXOKmlO\nhoRhHdg7sb0HeHKDNZ+rqp9V1XeBs4xCIWkHGhKGh4F9Sa5LciVwGFiZWvPPwJsAklzD6KPFuVkO\nKml+tgxDVT0H3AU8CDwOnKiq00nuTXJwvOxB4IdJzgAPAX9eVT/crqElba9UTZ8umI/l5eVaXV1d\nyPeW/r9I8khVLV/s47zyUVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWG\nQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZB\nUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUjMoDEkOJDmbZC3J3RdY95YklWR5diNKmrctw5DkCuAYcBuw\nHziSZP8G664G/gz4+qyHlDRfQ44YbgLWqupcVT0LPAAc2mDdB4D7gJ/McD5JCzAkDLuBJya218f7\nfiHJjcDeqvr8hZ4oydEkq0lWz58/f9HDSpqPIWHIBvvqF3cmLwI+BLx3qyeqquNVtVxVy0tLS8On\nlDRXQ8KwDuyd2N4DPDmxfTVwA/DlJN8DbgZWPAEp7VxDwvAwsC/JdUmuBA4DKz+/s6qeqaprqura\nqroWOAUcrKrVbZlY0rbbMgxV9RxwF/Ag8DhwoqpOJ7k3ycHtHlDS/O0asqiqTgInp/bds8naW174\nWJIWySsfJTWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1\nhkFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWG\nQVJjGCQ1hkFSMygMSQ4kOZtkLcndG9z/niRnkjyW5EtJXj37USXNy5ZhSHIFcAy4DdgPHEmyf2rZ\no8ByVf0W8FngvlkPKml+hhwx3ASsVdW5qnoWeAA4NLmgqh6qqh+PN08Be2Y7pqR5GhKG3cATE9vr\n432buRP4wkZ3JDmaZDXJ6vnz54dPKWmuhoQhG+yrDRcmdwDLwAc3ur+qjlfVclUtLy0tDZ9S0lzt\nGrBmHdg7sb0HeHJ6UZJbgfcBb6yqn85mPEmLMOSI4WFgX5LrklwJHAZWJhckuRH4B+BgVT01+zEl\nzdOWYaiq54C7gAeBx4ETVXU6yb1JDo6XfRD4VeAzSb6ZZGWTp5O0Awz5KEFVnQROTu27Z+L2rTOe\nS9ICeeWjpMYwSGoMg6TGMEhqDIOkxjBIagyDpMYwSGoMg6TGMEhqDIOkxjBIagyDpMYwSGoMg6TG\nMEhqDIOkxjBIagyDpMYwSGoMg6TGMEhqDIOkxjBIagyDpMYwSGoMg6TGMEhqDIOkxjBIagyDpMYw\nSGoMg6TGMEhqDIOkZlAYkhxIcjbJWpK7N7j/l5N8enz/15NcO+tBJc3PlmFIcgVwDLgN2A8cSbJ/\natmdwNNV9evAh4C/nvWgkuZnyBHDTcBaVZ2rqmeBB4BDU2sOAR8b3/4s8OYkmd2YkuZp14A1u4En\nJrbXgd/ZbE1VPZfkGeDlwA8mFyU5Chwdb/40yXcuZegFuYapn+cytpNmhZ01706aFeA3LuVBQ8Kw\n0W/+uoQ1VNVx4DhAktWqWh7w/S8LO2nenTQr7Kx5d9KsMJr3Uh435KPEOrB3YnsP8ORma5LsAl4K\n/OhSBpK0eEPC8DCwL8l1Sa4EDgMrU2tWgD8e334L8C9V1Y4YJO0MW36UGJ8zuAt4ELgC+GhVnU5y\nL7BaVSvAPwGfSLLG6Ejh8IDvffwFzL0IO2nenTQr7Kx5d9KscInzxl/skqZ55aOkxjBIarY9DDvp\ncuoBs74nyZkkjyX5UpJXL2LOiXkuOO/EurckqSQL+zPbkFmTvHX8+p5O8sl5zzg1y1bvhVcleSjJ\no+P3w+2LmHM8y0eTPLXZdUEZ+fD4Z3ksyeu3fNKq2rYvRicr/x14DXAl8C1g/9SaPwU+Mr59GPj0\nds70Amd9E/Ar49vvXNSsQ+cdr7sa+ApwCli+XGcF9gGPAr823n7F5fzaMjqp987x7f3A9xY47+8B\nrwe+s8n9twNfYHS90c3A17d6zu0+YthJl1NvOWtVPVRVPx5vnmJ0TceiDHltAT4A3Af8ZJ7DTRky\n69uBY1X1NEBVPTXnGScNmbeAl4xvv5R+bc/cVNVXuPB1Q4eAj9fIKeBlSV55oefc7jBsdDn17s3W\nVNVzwM8vp563IbNOupNRhRdly3mT3AjsrarPz3OwDQx5ba8Hrk/y1SSnkhyY23TdkHnfD9yRZB04\nCbx7PqNdkot9bw+6JPqFmNnl1HMweI4kdwDLwBu3daILu+C8SV7E6H+6vm1eA13AkNd2F6OPE7cw\nOhL71yQ3VNV/b/NsGxky7xHg/qr6myS/y+g6nhuq6n+2f7yLdtH/xrb7iGEnXU49ZFaS3Aq8DzhY\nVT+d02wb2Wreq4EbgC8n+R6jz5YrCzoBOfR98Lmq+llVfRc4yygUizBk3juBEwBV9TXgxYz+g9Xl\naNB7+3m2+aTILuAccB3/dxLnN6fWvIvnn3w8saATOENmvZHRSal9i5jxYuedWv9lFnfycchrewD4\n2Pj2NYwOfV9+Gc/7BeBt49uvG/9DywLfD9ey+cnHP+T5Jx+/seXzzWHg24F/G/+Det94372MfuPC\nqLSfAdaAbwCvWeCLu9WsXwT+C/jm+GtlUbMOmXdq7cLCMPC1DfC3wBng28Dhy/m1ZfSXiK+Oo/FN\n4A8WOOungO8DP2N0dHAn8A7gHROv7bHxz/LtIe8DL4mW1Hjlo6TGMEhqDIOkxjBIagyDpMYwSGoM\ng6TmfwEval/UlBeDXwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f955d36b8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(inverse_normalization(X_disc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  97/3500 [..............................] - ETA: 1:34:52 - D logloss: 0.6962 - G tot: 10.9239 - G L1: 0.9254 - G logloss: 1.6694"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "for e in range(nb_epoch):\n",
    "    progbar = generic_utils.Progbar(epoch_size)\n",
    "    start = time.time()\n",
    "    for _ in range(n_batch_per_epoch):\n",
    "        step += 1\n",
    "        image_batch, profile_batch = next(train_generator)\n",
    "        # Create a batch to feed the discriminator model\n",
    "        X_disc, y_disc = get_disc_batch(image_batch, profile_batch, \n",
    "                                        generator, step, patch_size,label_smoothing=True)\n",
    "        \n",
    "        # Update the discriminator\n",
    "        disc_loss = discriminator.train_on_batch(X_disc, y_disc)\n",
    "        \n",
    "        # Create a batch to feed the generator model\n",
    "        y_gen = np.zeros((image_batch.shape[0], 2), dtype=np.uint8)\n",
    "        y_gen[:, 1] = 1\n",
    "        \n",
    "        # Freeze the discriminator\n",
    "        discriminator.trainable = False\n",
    "        gen_loss = gan.train_on_batch(image_batch, [profile_batch, y_gen])\n",
    "        # Unfreeze the discriminator\n",
    "        discriminator.trainable = True\n",
    "        \n",
    "        # save and show the training progress\n",
    "        loss_df = loss_df.append({\n",
    "                \"step\" : step,\n",
    "                \"epoch\" : e,\n",
    "                \"train D logloss\" : disc_loss,\n",
    "                \"train G tot\" : gen_loss[0],\n",
    "                \"train G L1\" : gen_loss[1],\n",
    "                \"train G logloss\": gen_loss[2],\n",
    "            },ignore_index=True)\n",
    "        \n",
    "        progbar.add(batch_size, values=[(\"D logloss\", disc_loss),\n",
    "                                        (\"G tot\", gen_loss[0]),\n",
    "                                        (\"G L1\", gen_loss[1]),\n",
    "                                        (\"G logloss\", gen_loss[2])])\n",
    "        \n",
    "        # Save images for Visualization\n",
    "        if step % (n_batch_per_epoch / 4) == 0:\n",
    "            # Get new images from validation\n",
    "            filename = \"{}_{}\".format(e, step//nb_epoch)\n",
    "            sample_path = os.path.join(sample_dir,filename)\n",
    "            \n",
    "            image_sample, profile_sample = gen_sample(train_generator, nb_sample)\n",
    "            plot_generated_batch(image_sample,profile_sample,generator,sample_path+\"_train.png\")\n",
    "            image_sample, profile_sample = gen_sample(test_generator, nb_sample)\n",
    "            plot_generated_batch(image_sample,profile_sample,generator,sample_path+\"_test.png\")\n",
    "            loss_df.to_csv(os.path.join(sample_dir,\"loss.csv\"))\n",
    "        \n",
    "    if e % 10 == 0:\n",
    "        gen_weights_path = os.path.join(weights_dir,\"gen_weights_epoch{:03}.h5\".format(e))\n",
    "        generator.save_weights(gen_weights_path, overwrite=True)\n",
    "\n",
    "        disc_weights_path = os.path.join(weights_dir,\"disc_weights_epoch{:03}.h5\".format(e))\n",
    "        discriminator.save_weights(disc_weights_path, overwrite=True)\n",
    "\n",
    "        gan_weights_path = os.path.join(weights_dir,\"gan_weights_epoch{:03}.h5\".format(e))\n",
    "        gan.save_weights(gan_weights_path, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inverse_normalization(X):\n",
    "    # value range : (-1,1) => (0,1)\n",
    "    res = (X + 1.) / 2.\n",
    "    return np.clip(res, 0.,1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_generated_batch(image_sample, profile_sample, generator, plot_path):\n",
    "    nb_sample = image_sample.shape[0]\n",
    "    rows = []\n",
    "    for i in range(nb_sample):\n",
    "        image = inverse_normalization(np.squeeze(image_sample[i]))\n",
    "        real_profile = np.squeeze(inverse_normalization(profile_sample[i]))\n",
    "        y = generator.predict(image_sample[i:i+1])\n",
    "        fake_profile = np.squeeze(inverse_normalization(y))\n",
    "        real_profile = np.stack([real_profile]*3,axis=-1)\n",
    "        fake_profile = np.stack([fake_profile]*3,axis=-1)\n",
    "        print(image.shape,real_profile.shape,fake_profile.shape)\n",
    "        rows.append(np.concatenate([image, real_profile, fake_profile],axis=1))\n",
    "    sample = np.concatenate(rows,axis=0)\n",
    "    Image.fromarray((sample * 255).astype(np.uint8)).save(plot_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
