{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import math\n",
    "import os\n",
    "\n",
    "import keras.backend as K\n",
    "from keras import models, layers, optimizers\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코딩셰프의 3분 딥러닝 케라스 맛에 구현된 코드들이 가독성이 좋고, \n",
    "일반적인 상황에 적용하기 좋다고 판단되어서, 예시 코드를 적어둠\n",
    "이를 바탕으로 다른 모델도 적용할 예정\n",
    "\n",
    "Reference : \n",
    "* 코딩셰프의 3분 딥러닝 케라스맛"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_image_data_format('channels_first')\n",
    "print(K.image_data_format())\n",
    "\n",
    "def mse_4d(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true), axis=(1,2,3))\n",
    "\n",
    "def mse_4d_tf(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_pred - y_true), axis=(1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(models.Sequential):\n",
    "    def __init__(self, input_dim=64):\n",
    "        '''\n",
    "        self, self.generator, self.discriminator are all models\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        self.generator = self.GENERATOR()\n",
    "        self.discriminator = self.DISCRIMINATOR()\n",
    "        self.add(self.generator)\n",
    "        self.discriminator.trainable = False\n",
    "        self.add(self.discriminator)\n",
    "        \n",
    "        self.compile_all()\n",
    "        \n",
    "    def compile_all(self):\n",
    "        # Compiling state\n",
    "        d_optim = optimizers.SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
    "        g_optim = optimizers.SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
    "        \n",
    "        self.generator.compile(loss=mse_4d_tf, optimizer='SGD')\n",
    "        self.compile(loss='binary_crossentropy', optimizer=g_optim)\n",
    "        self.discriminator.trainable = True\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=d_optim)\n",
    "        \n",
    "    def GENERATOR(self):\n",
    "        input_dim = self.input_dim \n",
    "        \n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Dense(1024, activation='tanh',\n",
    "                              input_dim=input_dim))\n",
    "        model.add(layers.Dense(128 * 7 * 7, activation='tanh'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Reshape((128,7,7), input_shape=(128 * 7 * 7,)))\n",
    "        model.add(layers.UpSampling2D(size=(2,2)))\n",
    "        model.add(layers.Conv2D(64, (5,5), padding='same',\n",
    "                               activation='tanh'))\n",
    "        model.add(layers.UpSampling2D(size=(2,2)))\n",
    "        model.add(layers.Conv2D(1, (5,5), padding='same',\n",
    "                               activation='tanh'))\n",
    "        return model\n",
    "    \n",
    "    def DISCRIMINATOR(self):\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Conv2D(64, (5,5), padding='same',\n",
    "                               activation='tanh',\n",
    "                               input_shape = (1, 28, 28)))\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(1024, activation='tanh'))\n",
    "        model.add(layers.Dense(1, activation='sigmoid'))\n",
    "        return model\n",
    "    \n",
    "    def get_z(self, ln):\n",
    "        input_dim = self.input_dim\n",
    "        return np.random.uniform(-1, 1, (ln, input_dim))\n",
    "    \n",
    "    def train_both(self, x):\n",
    "        ln = x.shape[0]\n",
    "        # First Trial for Training Discriminator\n",
    "        z = self.get_z(ln)\n",
    "        w = self.generator.predict(z, verbose = 0)\n",
    "        xw = np.concatenate((x,w))\n",
    "        y2 = [1] * ln + [0] * ln\n",
    "        d_loss = self.discriminator.train_on_batch(xw, y2)\n",
    "        # Second trial for training Generator\n",
    "        z = self.get_z(ln)\n",
    "        self.discriminator.trainable = False\n",
    "        g_loss = self.train_on_batch(z, [1] * ln)\n",
    "        self.discriminator.trainable = True\n",
    "        \n",
    "        return d_loss, g_loss\n",
    "    \n",
    "# 합성곱 계층 GAN 학습 수행\n",
    "def combine_images(generated_images):\n",
    "    num = generated_images.shape[0]\n",
    "    width = int(math.sqrt(num))\n",
    "    height = int(math.ceil(float(num) / width))\n",
    "    shape = generated_images.shape[2:]\n",
    "    image = np.zeros((height * shape[0], width * shape[1]),\n",
    "                      dtype=generated_images.dtype)\n",
    "    for index, img in enumerate(generated_images):\n",
    "        i = int(index / width)\n",
    "        j =index % width\n",
    "        image[i*shape[0]:(i+1)*shape[0],\n",
    "              j*shape[1]:(j+1)*shape[1]] = img[0,:,:]\n",
    "    return image\n",
    "\n",
    "def get_x(X_train, index, BATCH_SIZE):\n",
    "    return np.expand_dims(X_train[index*BATCH_SIZE:(index+1)*BATCH_SIZE],axis=1)\n",
    "\n",
    "def save_images(generated_images, output_fold, epoch, index):\n",
    "    image = combine_images(generated_images)\n",
    "    image = (image * 127.5 + 127.5).astype(np.uint8)\n",
    "    Image.fromarray(image).save(output_fold + \"/\" + str(epoch) + \"_\" + str(index) + \".png\")\n",
    "\n",
    "def load_data(n_train):\n",
    "    (X_train, y_train), (_, _) = mnist.load_data()\n",
    "    return X_train[:n_train]\n",
    "\n",
    "def train(args):\n",
    "    BATCH_SIZE = args.batch_size\n",
    "    epochs = args.epochs\n",
    "    output_fold = args.output_fold\n",
    "    input_dim = args.input_dim\n",
    "    n_train = args.n_train\n",
    "\n",
    "    os.makedirs(output_fold, exist_ok=True)\n",
    "    print(\"output_fold is\", output_fold)\n",
    "\n",
    "    X_train = load_data(n_train)\n",
    "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "    \n",
    "    gan = GAN(input_dim)\n",
    "    \n",
    "    d_loss_ll = []\n",
    "    g_loss_ll = []\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch is\", epoch)\n",
    "        print(\"Number of Batches\", int(X_train.shape[0] / BATCH_SIZE))\n",
    "        \n",
    "        d_loss_l = []\n",
    "        g_loss_l = []\n",
    "        for index in range(int(X_train.shape[0] / BATCH_SIZE)):\n",
    "            x = get_x(X_train, index, BATCH_SIZE)\n",
    "            \n",
    "            d_loss, g_loss = gan.train_both(x)\n",
    "            \n",
    "            d_loss_l.append(d_loss)\n",
    "            \n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            z = gan.get_z(x.shape[0])\n",
    "            w = gan.generator.predict(z, verbose=0)\n",
    "            save_images(w, output_fold, epoch, 0)\n",
    "            d_loss_ll.append(d_loss_l)\n",
    "            g_loss_ll.append(g_loss_l)\n",
    "            \n",
    "        gan.generator.save_weights(output_fold + \"/generator\", True)\n",
    "        gan.discriminator.save_weights(output_fold + \"/discriminator\", True)\n",
    "        \n",
    "        np.savetxt(output_fold + '/d_loss', d_loss_ll)\n",
    "        np.savetxt(output_fold + '/g_loss', g_loss_ll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class obj(object):\n",
    "    def __init__(self, d):\n",
    "        for a, b in d.items():\n",
    "            if isinstance(b, (list, tuple)):\n",
    "                setattr(self, a, [obj(x) if isinstance(x, dict) else x for x in b])\n",
    "            else:\n",
    "                setattr(self, a, obj(b) if isinstance(b, dict) else b)\n",
    "\n",
    "args = {\n",
    "    'batch_size': 16,\n",
    "    'epochs' : 1000,\n",
    "    'output_fold' : 'GAN_OUT',\n",
    "    'input_dim' : 10,\n",
    "    'n_train' : 32\n",
    "}                \n",
    "args = obj(args)\n",
    "\n",
    "train(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
